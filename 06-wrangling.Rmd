# Processamento dos dados {#processamento}

<!--- preview_chapter("06-wrangling.Rmd") --->

_Davi Moreira, Mônica Rocabado_

&nbsp;

<!--- $~$  add line space --->

A análise de conteúdo só é possível através da transformação do texto bruto em estruturas de dados convenientes para análise. Esta etapa é fundamental e deve ser feita com cuidado para evitar erros futuros.
Temos basicamente três estruturas num texto:

- *Tokens*: O texto pode ser armazenado em *n-grams* ou cadeias de caracteres (*strings*), por exemplo a frase "projeto de lei", em n-gram de tamanho 1, ou seja unigram, se torna "projeto" "de" "lei".
- *Corpus*: Esses tipos de objetos geralmente contêm strings brutas com metadados sobre os documentos. Também pode ser descrito como uma coleção de documentos.
- *Matriz de documentos e termos (DFM ou DTM)*: é uma matriz esparsa que descreve uma coleção (ou seja, um corpus) de documentos com uma linha para cada documento e uma coluna para cada termo. Em geral, as células de uma DTM são preenchidas pela frequência de palavras.

A análise do conteúdo do texto como dado exige versatilidade na transformação entre estruturas. Os pacotes que veremos permitem essa versatilidade.

## Os pacotes

Para este capítulo utilizaremos o pacote `tidytext` e `quanteda`, ambos estruturais para análise de conteúdo. Inicialmente trabalharemos com o pacote `tidytext`, devido sua convergência com o pacote `tidyverse`. Em seguida, utilizaremos o pacote `quanteda`, que possui mais funcionalidades de [análise](https://quanteda.io/articles/pkgdown/comparison.html). 

## Tokens

### O formato tidy text (texto arrumado)^[Toda essa seção está baseada no livro [Text Mining with R](https://www.tidytextmining.com/)]

Usar os princípios do _tidy_ _text_ é uma maneira poderosa de tornar o processamento de dados mais ágil e eficaz. Conforme [Wickham (2014)](https://www.tidytextmining.com/tidytext.html#ref-tidydata), os dados organizados têm uma estrutura específica:

- Cada variável é uma coluna
- Cada observação é uma linha
- Cada tipo de unidade de observação é uma tabela

<center>
![O formato tidy](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)
</center>

Assim, o formato de "texto arrumado" segue a mesma estrutura apresentada, na qual cada linha/observação possui uma unidade de texto significativa, também chamado por *token*, estes organizados em uma coluna/variável. Reforçando, o *token* então é uma unidade de texto significativa, podendo ser uma única palavra, um conjunto de palavras, uma frase ou um parágrafo. Para obte-lo se deve realizar o processo de *tokenização*, em que se divide o texto em tokens, como veremos asseguir.


### Função `unnest_tokens`

Vejamos um trecho de "Canção do Exílio", poesia romântica de Gonçalves Dias escrita em 1843. No caso, estamos criando um vetor.

```{r, echo = T, eval = F, warning = F, message = F}
text <- c("Minha terra tem palmeiras", "Onde canta o Sabiá", 
          "As aves, que aqui, gorjeiam", "Não gorjeiam como lá")

knitr::kable(text)
```


Para transformar o vetor de strings em formato tidy text dataset, precisamos, primeiro, criar um data.frame`, através da função tibble. Na função abaixo estamos declarando o nome das colunas e o valor contido nelas, por exemplo "line" indica o id ou número da linha, e "text" é a coluna que contém cada observação do vetor "text" que criamos anteriormente. 

```{r, echo = T, eval = F, warning = F, message = F, cache=T}
# carregando pacotes ----
library(dplyr)

text_df <- tibble(line = 1:4, text = text)

knitr::kable(text_df)
```


Um objeto `tibble` é uma classe moderna de `data.frames` dentro do `R`, disponível nos pacotes `dplyr` e `tibble`, que possui um método de impressão conveniente, não converte strings em fatores e não usa nomes de linhas. Tibbles são ótimos para uso com funções, pacotes e ferramentas tidy.

Contudo, nosso objeto `tibble` ainda não está coerente com a definição de tidy text que apresentamos. Para tanto, temos que converter nosso objeto em outro que atenda a condição *one-token-per-document-per-row*, logo cada token unigram, ou seja palavra, é um valor indicado por linha. A função `unnest_tokens` presente no pacote realiza este processo de tokenização. Abaixo estamos criando a coluna "word", que irá conter uma palavra por linha através da coluna "text" que contém nosso texto.

```{r, echo = T, eval = F, warning = F, message = F}
# carregando pacotes ----
library(tidytext)

text_token <- text_df %>%
  unnest_tokens(word, text)


knitr::kable(text_token)
```


### Tidying a aprovação do impeachment da Presidenta Dilma Rousseff {#tidyDilma}

```{r, include=FALSE}
library(here)

load(here("data/impeachment-dilma-dados-filter.rda"))
```

Vamos utilizar como exemplo os discursos proferidos pelos parlamentares da Câmara dos Deputados na sessão de Impeachment da Presidenta Dilma Rousseff em Abril de 2016. Cada linha contém um discurso por parlamentar, podendo haver mais de um discurso por deputado. Para obter estes dados você precisa utilizar o pacote `txt4cs` que acompanha o livro.


```{r, echo = T, eval = F, warning = F, message = F}
# carregando pacotes ----
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
devtools::install_github("davi-moreira/txt4cs-pkg")
library(txt4cs)

impeachment_dilma <- txt4cs::impeachment_dilma

```

Agora iremos *tokenizar* os discursos dentro do formato Tidy, permitido ainda a associação do token com demais dados correspondentes, no caso o nome do deputado e o partido que participa.
A função `unnest_tokens`, como já colocado, permite a tokenização, no entanto deve-se indicar dois argumento principais para seu funcionamento: o nome da coluna que será criada, esta posta abaixo como `word` e indicar qual coluna se deseja realizar a tokenização, no caso `text`. O padrão da função é a divisão do texto por uma palavra, ou unigram, sendo possível sua alteração caso se queira um tamanho maior de n-grams.

```{r, echo = T, eval = F, warning = F, message = F}

tidy_impeachment <- impeachment_dilma %>%
  unnest_tokens(word, text)


head(tidy_impeachment[,5], 30)

```


Como é possível notar, há uma diversidade de conectores e termos que não agregram valor a nossa análise, estes são chamados de `stopwords`, que serão removidos da base. Percebe-se também que no texto o nome do partido aparece sempre seguido do nome do deputado, dessa forma também será filtrado da análise. Para remover essas palavras vamos utilizar a função `anti_join`, que irá remover os objetos que criamos que contém as `stopwords` e o nome dos partidos dos nossos tokens.

```{r, echo = T, eval = F, warning = F, message = F}

#stopwords
library(quanteda)
stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))
stop_w <- stop_w %>% add_row(word = c("sr", "25", "voto", "presidente", "votos", "total", "deputado"))

#remover o nome do partido
partido <- impeachment_dilma$partido %>%
  tibble() %>%
  distinct() %>%
  rename("word" = ".") %>%
  mutate(word = str_to_lower(word))

#retirar do corpus as stopwords
tidy_impeachment <- tidy_impeachment %>% 
  anti_join(stop_w) %>%
  anti_join(partido)

head(tidy_impeachment[,5], 30)

```

Com a informação já no formato tidy já é possível iniciar algumas análises com os dados, como a frequência de palavras:

```{r, echo = T, eval = F, warning = F, message = F}
tidy_impeachment %>%
  count(word, sort = TRUE) 
```

Também pode-se visualizar as frequências geradas
```{r, eval = F, message = F}
tidy_impeachment %>%
  count(word, sort = TRUE) %>%
  mutate(word = fct_reorder(word, n)) %>%
  filter(n > 160) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(x="") 
```

Com o dado em formato Tidy, é possível comparar as palavras mais utilizadas por partido:

```{r, echo = T, eval = F, warning = F, message = F}
tidy_impeachment %>%
  count(word, partido, sort = TRUE) %>%
  filter(n > 55) %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(fill = partido, x = word, y= n)) +
  geom_col() +
  coord_flip() +
  labs(x="") +
  scale_fill_brewer()
```

Avançando um pouco mais, já seria possível comparar o uso de palavras por diferentes deputados. Vamos comparar os deputados do PT, PSOL e do PSDB. Dessa forma, como o exemplo acima indica, já possuímos os dados em formato tidy, vamos selecionar os termos por partido.


```{r, echo = T, eval = F, warning = F, message = F}
impeachment_pt <- impeachment_dilma %>% 
  filter(partido == "PT")

impeachment_psdb <- impeachment_dilma %>% 
  filter(partido == "PSDB")

impeachment_psol <- impeachment_dilma %>% 
  filter(partido == "PSOL")
```

Com os dados filtrados, vamos transforma-los em um dataframe 

```{r, echo = T, eval = F, warning = F, message = F}
impeachment_pt <- tibble(line = 1:nrow(impeachment_pt),
                         text = impeachment_pt$text)  

impeachment_psdb <- tibble(line = 1:nrow(impeachment_psdb), 
                           text = impeachment_psdb$text)  

impeachment_psol <- tibble(line = 1:nrow(impeachment_psol), 
                           text = impeachment_psol$text)  
````

Após esse processo de seleção do dado de texto por partido, vamos tokenizar a informação através da função `unnest_tokens`, do pacote `tidyverse` e retirar as stopwords por meio da função `anti_join` do pacote `dplyr`.

```{r, echo = T, eval = F, warning = F, message = F}
#Tokenizando

tidy_impeachment_pt <- impeachment_pt %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_w) 

tidy_impeachment_psdb <- impeachment_psdb %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_w) 
  
tidy_impeachment_psol <- impeachment_psol %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_w) 
````

Para termos um gráfico que compare a proporção de palavras utilizadas pelo PSOL e PSDB em relação ao PT, temos que:

 - Unir as bases tratadas em uma única
 - Medir a frequência dos termos utilizados por autor
 - Criar uma coluna com os tokens
 - Criar uma coluna com a frequência utilizada pelo PT
 - Criar uma coluna outra contendo a frequência de uso dos termos pelo PSDB e PSOL
 - Criar uma coluna que identifique o partido, entre PSDB e PSOL.

```{r, echo = T, eval = F, warning = F, message = F}
# frequencia de palavras
library(tidyr)

frequency <- bind_rows(mutate(tidy_impeachment_psol, author = "PSOL"),
                       mutate(tidy_impeachment_psdb, author = "PSDB"),
                       mutate(tidy_impeachment_pt, author = "PT")) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `PSOL`:`PSDB`)

knitr::kable(head(frequency,5))

````

Vamos produzir um gráfico que demonstre os termos mais relativamente utilizados pelo PT, PSDB e PSOL no contexto do discurso do impeachment. Para conseguirmos realizar o comparativo dos três partidos, vamos colocar os valores de PT como eixo `X` e `facet` pelos autores PSDB e PSOL. Dessa forma, no gráfico, quanto mais próximo um termo do eixo `y`, mais ele foi relativamente utilizado pelo PSDB ou PSOL e quanto mais próximo ao eixo x, mais ele foi utilizado por um deputado do PT.

```{r, echo = T, eval = F, warning = F, message = F}  
# gráfico
ggplot(frequency, aes(x = proportion, y = `PT`,
                      color = abs(`PT` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  scale_color_gradient(limits = c(0, 0.001),
                       low = "darkslategray4", high = "gray7") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "PT", x = NULL)

```

Podemos quantificar quão diferente é o vocabulário através de um simples teste de correlação.

```{r, echo = T, eval = F, warning = F, message = F}

cor.test(data = frequency[frequency$author == "PSDB",],
         ~ proportion + `PT`)

```

## Corpus

O pacote `tidyverse` não possui uma função que gera um corpus, assim iremos utilizar o pacote `quanteda` que permite trabalhar com Tokens, Corpus e DFMs. Um ponto positivo em relação ao tidyverse é a possibilidade de gerar corpus e demais análises estas estando relacionadas ao documento que a originou. 

O uso do quanteda, segue o seguinte fluxograma:

<center>
![quanteda_flow](images/quanteda-workflow.png){width=500px}
</center>

É possível criar um *corpus* a partir de diferentes fontes de dados:

1. Um vetor de caracteres que consista em um documento por elemento;

2. Uma matriz de dados que consista em um vetor de caracteres para os documentos e vetores adicionais para variáveis no nível do documento;

3. Um objeto de classe `VCorpus` ou `SimpleCorpus` criado pelo pacote `tm`;

4. Um objeto de palavras-chave no contexto construído por `kwic()`;

Utilizando a base de dados com as falas proferidas no dia da aprovação do impeachment da Presidenta Dilma Rousseff pela Câmara dos Deputados, nesta seção focaremos em como construir um corpus a partir de uma matriz de dados. A base de dados é a mesma utiliada no item anterior, podendo ser carregada através do pacote `txt4cs`

```{r, echo = T, eval = F, warning = F, message = F}
# carregando pacotes ----
library(quanteda)
library(readtext)
library(stringr)
library(txt4cs)

impeachment_dilma <- txt4cs::impeachment_dilma
````

Inicialmente devemos transformar o data.frame em um corpus, utilizando a função  `corpus()`do pacote `quanteda`

````{r, echo = T, eval = F, warning = F, message = F}

corp <- corpus(impeachment_dilma)
summary(corp, 5)
```

### Filtrando corpus: `corpus_subset()`

A função `corpus_subset()` permite selecionar documentos em um corpus com base em variáveis no nível do documento. No caso iremos selecionar o corpus referente ao PSDB e PT.

```{r, echo = T, eval = F, warning = F, message = F}

corp_ptpsdb <- corpus_subset(corp, partido %in% c('PT', 'PSDB'))

summary(corp_ptpsdb, 5)

```

### Trocando a unidade de texto no corpus: `corpus_reshape()`

A função `corpus_reshape()` permite alterar a unidade de textos entre documentos, parágrafos e frases. Os textos podem ser restaurados para a unidade original mesmo que o corpus seja modificado por outras funções. 
Dessa forma, nos exemplos abaixo podemos mudar o corpus para formato de sentenças. A sentença segundo a função termina no ponto final e dá início a outra sentença.

```{r, echo = T, eval = F, warning = F, message = F}
corp_sent <- corpus_reshape(corp, to = 'sentences')

corp_sent[11:12]
````

Ou alterá-lo para um formato de documento, restaurando-os ao formato original:

```{r, echo = T, eval = F, warning = F, message = F}
# restaurando documentos originais ----
corp_documents <- corpus_reshape(corp_sent, to = 'documents')

corp_documents[2]

```

## Tokens e Corpus
### Obtendo Tokens a partir de um Corpus: `tokens()`

A função `tokens()`, como já vimos, segmenta o texto em unidades de textos significativas.Esta função no pacote `quanteda` já permite a remoção de espaços em branco e demais separadores de texto automaticamente, para isso é necessário escrever o argumento, como exemplificado abaixo na removação automática de pontuação. Usaremos o corpus gerado do item anterior.  

```{r, echo = T, eval = F, warning = F, message = F}

toks <- tokens(corp, remove_punct = TRUE) #tokenizando e removendo a pontuação

head(toks[[1]], 50)
````

### Palavras-chave e seu contexto: `kwic()`

Através dessa função presente no pacote `quanteda` é possível identificar o contexto imediato de um termo ou conjunto de palavras-chaves. No exemplo abaixo estamos buscando o contexto de palavras relacionadas a "golpe":
 
```{r, echo = T, eval = F, warning = F, message = F}
kw <- kwic(toks, pattern =  'golp*')

knitr::kable(head(kw, 5))
```

Também é possível buscar mais de um termo por linha de código, assim como selecionar a quantidade de caracteres que apareçam com o termo procurado.

```{r, echo = T, eval = F, warning = F, message = F}
kw <- kwic(toks, pattern =  c("pela", "pelo"), window = 7)


knitr::kable(head(kw, 5))
```

Caso queira identificar o contexto de frases, você deve dentro de `phrase` escrever uma frase respeitando os espaços em branco. 

````{r, echo = T, eval = F, warning = F, message = F}
# caso de uma frase inteira ----
kw_frase <- kwic(toks, pattern =  phrase('não vai ter golp*'))

knitr::kable(kw_frase)
```


### Selecionando tokens

Como já colocado, a função `tokens` no pacote `quanteda` apenas remove separadores de texto e outros caracteres não textuais automaticamente se expressados no código. Para retirada de stopwords pode se fazer uso das funções `tokens_select` e `tokens_remove`, que são equivalentes. Novamente utilizaremos o corpus previamente construído.

```{r, echo = T, eval = F, warning = F, message = F}
#Construção de tokens sem pontuações
toks <- tokens(corp, remove_punct = T, remove_numbers = T)

# Removendo as stopwords
#opção 1
toks_nostop <- tokens_select(toks, pattern = stopwords('pt'),
                             selection = 'remove')

#opção 2 
toks_nostop <- tokens_remove(toks_nostop, pattern = c('SR', 'PRESIDENTE', 
                                                      'Esclarecimentos'))
head(toks_nostop[[1]], 50)

```

Com a função `tokens_select` também é possível selecionar palavras-chaves para análise e construindo um novo objeto. Por exemplo, para analisar palavras que aparecem junto de termos determinados, e sua posição original no texto, se pode utilizar respectivamente `window` e `padding` ao escrever o código:

```{r, echo = T, eval = F, warning = F, message = F}

toks_voto <- tokens_select(toks, pattern = c("votaç*", "voto"), padding = TRUE, window = 5)

head(toks_voto, 1)
```

## Stemming

Até aqui, mesmo que os procedimentos adotados tenham diminuído a dimensionalidade do acervo de documentos ao transformá-lo em uma sacola de palavras (*bag of words*), ainda é necessária a adoção de procedimentos que possam reduzir a complexidade do conteúdo a ser analisado.

Com esse objetivo, como apontado por Izumi e Moreira (2018) podemos pensar que determinado documento tenha em sua composição as seguintes palavras únicas: trabalho, trabalhador, trabalhista. Apesar de seus diferentes signficados, cada uma dessas palavras pode ser reduzida ao seu radical, _trabalh_, dando ao pequisador informações suficientes para sua análise e, assim reduzindo, o _n_ de três palavras únicas para uma palavra que tem a soma das frequências anteriores. 
Para garantir que palavras que variam apenas na flexão, número ou conjugação sejam consideradas iguais, reduzindo o número de dimensões contido no acervo, por meio da adaptação do algoritmo de Porter (1980) para o português já desenvolvida por diferentes projetos ([Snowball](http://snowball.tartarus.org/) e [NILC-USP](https://bit.ly/2OXkVS6)),podem ser obtidos os _stems_ das palavras restantes^[O processo de stemming é uma aproximação do processo de lematização, que reduz palavras às suas formas básicas.]. 

Chamamos esse procedimento de **Stemming**, o pacote `quanteda` possui a opção de stemming em português que usa o projeto Snowball através da função `tokens_wordstem()`, abaixo vamos utilizar o token já produzido no item acima.
                                  
```{r, echo = T, eval = F, warning = F, message = F}

token_stem <- tokens_wordstem(toks,language = "pt")


head(token_stem, 1)

```

### Gerando n-grams

O método de tokenização apresentado nos itens acima se chama bag-of-words, ou BOW, e como observado nas análises realizadas, não respeita a ordem em que as palavras foram escritas. 

Para algumas análises essa ordem é necessária e para realizá-la pode-se utilizar o método N-grams, gerando assim tokens de qualquer tamanho e respeitando a ordem em que aparecem no texto. Através da função `tokens_ngrams()` é possível utilizar esse método. 

Com `tokens_ngram()` você pode definir o tamanho que deseja que seus tokens sejam criados, tanto especificando um n-gram único, ou estabelecendo um limite de tamanho de n-grams que o próprio R irá identificar no texto.


```{r, echo = T, eval = F, warning = F, message = F}

toks <- tokens(corp, remove_punct = T, remove_numbers = T)

#criando n-grams
toks_ngram <- tokens_ngrams(toks, n = 2:4)

head(toks_ngram[[1]], 20)
```

O `tokens_compound` realiza processo similiar, mas com a diferença de gerar n-grams de forma seletiva, especificando o termo de análise desejada. Após a seleção, para análise do resultado, você deve filtrar pelo termo com `tokens_select`

```{r, echo = T, eval = F, warning = F, message = F}
#Gerar n-grams específicas
toks_neg_bigram <- tokens_compound(toks, pattern = phrase('não *'))

#Selecionar as n-grams geradas
toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase('não_*'))

#Resultado
head(toks_neg_bigram_select[[6]], 50)
```

## DFM: Matriz de documentos e termos

Vamos transformar nossos tokens em uma DFM (Document Feature Matrix), também chamada de DTM (Document Term Matrix). Relembrando que uma DFM é uma vetorização do texto, em que cada linha representa um documento e cada coluna um termo. Para isso iremos usar a função `dfm()`.


```{r, echo = T, eval = F, warning = F, message = F}

#Construindo um corpus e removendo stopwords
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = T)
toks <- tokens_select(toks, pattern = stopwords('pt'), selection = 'remove')
toks <- tokens_remove(toks, pattern = c('sr', 'presidente', 'esclarecimentos', 'quero', 'ser', 'srs', 'é', 'aqui'))

#Construindo um DFM
dfmat <- dfm(toks)
```

Você pode obter a quantidade de documentos e termos utilizando `ndoc` e `nfeat`
Abaixo vemos que temos 555 documentos
```{r, echo = T, eval = F, warning = F, message = F}
ndoc(dfmat)  # numero de documentos
```

Observamos que temos 6395 termos, ou features.
```{r, echo = T, eval = F, warning = F, message = F}
nfeat(dfmat)  # numero de features
```

Também é possível obter o nome dos documentos e das features com `docnames` e `featnames` e quais as features mais frequentes com `topfeatures`

Abaixo estamos combinando a função `head` com `docnames` para obtermos somente o nome dos 20 primeiros documentos.
```{r, echo = T, eval = F, warning = F, message = F}
head(docnames(dfmat), 20)  # ids dos documentos
```

Abaixo estamos combinando a função `head` com `featnames` para obtermos somente o nome das 20 primeiras features.
```{r, echo = T, eval = F, warning = F, message = F}
head(featnames(dfmat), 20)  # algumas features
```

Para verificar as 10 features mais frequentes podemos somente utilizar a função `topfeatures` e indicar quantas features queremos que retorne.
```{r, echo = T, eval = F, warning = F, message = F}
topfeatures(dfmat, 10)  # features mais frequentes
```


### Refinando a seleção de features

Você pode selecionar as features que tiveram uma frequência desejada para analisar através de `dfm_trim`. No caso abaixo, documentos com menos de 1% de frequência por features serão removidas, assim de 6320 features ficamos com 769 features.

```{r, echo = T, eval = F, warning = F, message = F}
dfmat_docfreq <- dfm_trim(dfmat, min_docfreq = 0.01, docfreq_type = "prop")

nfeat(dfmat_docfreq)
```

### Realizando Steam numa DFM
Caso não tenha realizado o stem através dos tokens e já possui uma dfm, é possível realizá-la também nesta etapa, através da função `dfm_wordstem`

```{r, echo = T, eval = F, warning = F, message = F}
dfmat_stem <- dfm_wordstem(dfmat)
```


### Agrupando documentos numa DFM

Para agrupar os documentos em uma DFM, você pode utilizar a função `dfm_group`, que irá uni-los baseado em um argumento. No entanto, para simplificar o seu código, você pode agrupar baseado em um argumento no momento de gerar um dfm.

No exemplo abaixo estamos agrupando a dfm por nome do orador, retornando 512 documentos.
```{r, echo = T, eval = F, warning = F, message = F}
# Por autor
dfmat_autor <- dfm(dfmat_docfreq, groups = "nomeOrador")

ndoc(dfmat_autor)
````

No exemplo abaixo estamos agrupando a dfm por nome do partido, retornando 26 documentos.
```{r, echo = T, eval = F, warning = F, message = F}
# Por partido ----
dfmat_party <- dfm(dfmat_docfreq, groups = "partido")

ndoc(dfmat_party)
```

## FCM: Matriz de co-ocorrência de termos

Retomando, uma FCM (Feature Co-occurrence Matrix) é uma matriz de co-ocorrência de termos, ou seja, essa função mensura a co-ocorrencia de features dentro  de um contexto definido previamente, retornando linhas e colunas com os termos e os valores a contagem de co-ocorrencia entre esses termos. O contexto pode ser um documento ou um conjunto de documentos.

Pode-se construir uma FCM a partir de uma DFM ou um objeto de tokens usando a função `fcm()`, assim demonstrando o número de co-ocorrências de um feature, seu comportamento é similiar a uma DFM.

```{r, echo = T, eval = F, warning = F, message = F}

fcmat <- fcm(dfmat)
head(fcmat, 5)

````

Por meio de um FCM é possível retornar as palavras que ocorrem mais frequentemente com `topfeatures()`, assim como visualizar uma rede semântica com a função `textplot_network()`, por exemplo.

```{r, echo = T, eval = F, warning = F, message = F}

feat <- names(topfeatures(fcmat, 50))
fcmat_select <- fcm_select(fcmat, pattern = feat, selection = "keep")
head(fcmat_select, 5)
````

Estabelecemos o tamanho da rede selecionando o DFM segundo as topfeatures criadas no objeto acima. Para plotar o gráfico das principais features utilizamos `textplot_network`, estabelecendo uma frequência mínima.

```{r, echo = T, eval = F, warning = F, message = F}
size <- log(colSums(dfm_select(dfmat, feat, selection = "keep")))
set.seed(144)
textplot_network(fcmat_select, min_freq = 0.95,
                 vertex_size = size / max(size) * 3)

```

