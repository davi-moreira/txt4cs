--- 
knit: 'bookdown::publish_book(render = "local")'
title: "_Text as Data_ para Ciências Sociais"
subtitle: 'guia prático com aplicações'
author: "Davi Moreira"
date: "Criado em 02-08-2019. Atualizado em: `r format(Sys.Date(), '%d-%m-%Y')`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [biblioteca.bib, book.bib, packages.bib]
biblio-style: apalike
github-repo: https://github.com/davi-moreira/txt4cs
link-citations: yes
description: "Compilação de métodos e técnicas para análise automatizada de conteúdo"
urlcolor: blue  
---

# Prefácio {-}

Placeholder


## Objetivo {-}
## Sobre o autor/organizador {-}
## Licença {-}
## Agradecimentos {-}

<!--chapter:end:index.Rmd-->


# Introdução {#intro}

Placeholder


## O `R` e o `RStudio`
## O Pacote `txt4cs` e outros
## Material de apoio
### Referências para processamento de sequências de caracteres com o `R`
### Referências em análise de conteúdo com o `R`:

<!--chapter:end:01-intro.Rmd-->


# Text as data: o texto como dado {#tada}

Placeholder


## Panorama da área
## Oportunidades
## Quadro geral de metodologias
## O processo de análise do texto como dado

<!--chapter:end:02-tada.Rmd-->


# R e o Processamento de Linguagem Natural {#regex}

Placeholder


## Encoding - Codificação de caracteres
## Encoding para remover acentos

<!--chapter:end:03-regex.Rmd-->


# Strings no `R` {#stringR}

Placeholder


## Strings e vetores
### O `R` é case sensitive
### Sequências de caracteres
### Operações básicas com vetores de strings
#### Atenção
### Caracteres e outros tipos de dados
## Strings e matrizes
## Strings e data.frames
## Strings e listas
## Processamento básico
### Contando caracteres
### `toupper()`, `tolower()`
#### Recortando strings:  `substr()`, `substring()`.  
#### União, Intersecção, Diferença, Igualdade
#### Elemento contido em
#### Ordenação
## O pacote `stringr` 
### Verificando o tamanho de uma string^[Assim como fizemos anteriormente com a função `nchar()`.]
### Identificando caracter numa posição específica.
### Incluindo caracter ou string numa posicao específica.
### Recortando uma string para obter parte da sequência de caracteres.
## Regular Expressions no `R`
### Identificação e Extração de padrão
### Substituição
### Âncoras

<!--chapter:end:04-string-R.Rmd-->


# Obtenção de conteúdo {#scrape}

Placeholder


## word, excel ou `.pdf`{#pdf}
### `.xlsx` 
### `.pdf` e `.doc`
## Webscraping
### Pacotes para raspagem de dados
### Etapas para raspagem de dados na web
### Código fonte 
### Obtenção de Código Fonte - Exemplo:
#### Atividade prática
## Web Services
### Obtenção de conteúdo via WS - Exemplo:
### a) obter os meta-dados dos discursos
### b) obter o conteúdo dos discursos (inteiror teor).**
#### Atividade prática
## Download de arquivos da web
## Twitter
## Imagens
## Áudio Transcrição

<!--chapter:end:05-obtencao-conteudo.Rmd-->

# Processamento dos dados {#processamento}

A análise de conteúdo só é possível através da transformação do texto bruto em estruturas de dados convenientes para o uso em pacotes. Esta etapa é fundamental e caso feita de forma errada pode levar a erros ou a impossibilidade da análise.
Temos basicamente quatro estruturas:

- *Tokens*: O texto pode ser armazenado em *n-grams* ou cadeias de caracteres (*strings*), por exemplo a frase "projeto de lei", em n-gram de tamanho 1, ou seja unigram, se torna "projeto" "de" "lei".
- *Corpus*: Esses tipos de objetos geralmente contêm strings brutas com metadados e detalhes adicionais. Também pode ser descrito como uma coleção de documentos.
- *Matriz de documentos e termos (DFM ou DTM)*: é uma matriz esparsa que descreve uma coleção (ou seja, um corpus) de documentos com uma linha para cada documento e uma coluna para cada termo. O valor na matriz é tipicamente contagem de palavras.

A análise do conteúdo do texto como dado exige versatilidade na transformação entre estruturas. Os pacotes vistos neste curso permitem essa versatilidade.


## Tokens

### O formato tidy text (texto arrumado)^[Toda essa seção está baseada no livro [Text Mining with R](https://www.tidytextmining.com/)]

Usar os princípios do _tidy_ _text_ é uma maneira poderosa de tornar o processamento de dados mais ágil e eficaz. Conforme [Wickham (2014)](https://www.tidytextmining.com/tidytext.html#ref-tidydata), os dados organizados têm uma estrutura específica:

- Cada variável é uma coluna
- Cada observação é uma linha
- Cada tipo de unidade de observação é uma tabela

<center>
![O formato tidy](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)
</center>

Assim, o formato de texto arrumado nada mais é do que uma tabela com um *token* por linha. Um *token* é uma unidade de texto significativa, como uma palavra, e *tokenização* é o processo de dividir o texto em tokens. Para facilitar a mineração de texto, o token armazenado em cada linha geralmente é uma única palavra, mas também pode ser um *n-grama*, uma frase ou um parágrafo.



### Função `unnest_tokens`

Vejamos um trecho de "Canção do Exílio", poesia romântica de Gonçalves Dias escrita em 1843. No caso, estamos criando um vetor.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
text <- c("Minha terra tem palmeiras", "Onde canta o Sabiá", 
          "As aves, que aqui, gorjeiam", "Não gorjeiam como lá")

```

```{r echo=FALSE}
knitr::kable(text)
```


Para transformar o vetor de strings num tidy text dataset, precisamos, primeiro, criar um data.frame`, através da função tibble, indicando a coluna linha com o número de linhas e a coluna text que irá conter o texto.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# carregando pacotes ----
library(dplyr)

text_df <- tibble(line = 1:4, text = text)

```

```{r echo=FALSE}
knitr::kable(text_df)
```


Um objeto `tibble` é uma classe moderna de `data.frames` dentro do `R`, disponível nos pacotes `dplyr` e `tibble`, que possui um método de impressão conveniente, não converte strings em fatores e não usa nomes de linhas. Tibbles são ótimos para uso com funções, pacotes e ferramentas tidy.

Contudo, nosso objeto `tibble` ainda não está coerente com a definição de tidy text que apresentamos. Para tanto, temos que converter nosso objeto em outro que atenda a condição *one-token-per-document-per-row*, logo cada token é um valor indicado por linha, no caso palavra. A função `unnest_tokens` presente no pacote realiza este processo de tokenização.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# carregando pacotes ----
library(tidytext)

text_token <- text_df %>%
  unnest_tokens(word, text)

```

```{r, echo=FALSE}
knitr::kable(text_token)
```


### Tidying a aprovação do impeachment da Presidenta Dilma Rousseff {#tidyDilma}

```{r include=FALSE}

load("~/R/txt4cs/data/impeachment-dilma-dados-filter.rda")
```

Vamos utilizar como exemplo os discursos proferidos pelos parlamentares da Câmara dos Deputados na sessão de Impeachment da Presidenta Dilma Rousseff em Abril de 2016. Cada linha contém um discurso por parlamentar, podendo haver mais de um discurso por deputado. Para obter estes dados você precisa utilizar o pacote `txt4cs` 

```{r, results = 'hide', echo = TRUE, eval = TRUE, warning = F, message = F}
# carregando pacotes ----
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
devtools::install_github("davi-moreira/txt4cs-pkg")
library(txt4cs)

impeachment_dilma <- txt4cs::impeachment_dilma

```

Tokenizando os dados com os discursos dentro do formato Tidy, permitido ainda a associação do token com demais dados correspondentes, no caso o nome do deputado e o partido que participa.
A função `unnest_tokens`, como já colocado, permite a tokenização, no entanto deve-se indicar dois argumento principais para seu funcionamento: o nome da coluna que será criada, esta posta abaixo como `word` e indicar qual coluna se deseja realizar a tokenização, no caso `text`. O padrão da função é a divisão do texto por palavra, ou unigram, sendo possível sua alteração caso se queira um tamanho maior de n-grams.

```{r, results='hide', echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}

tidy_impeachment <- impeachment_dilma %>%
  unnest_tokens(word, text)


head(tidy_impeachment[,5], 30)

```


Como é possível notar, há uma diversidade de conectores e termos que não agregram valor a nossa análise, estes são chamados de `stopwords`, que serão removidos da base. Percebe-se também que no texto o nome do partido aparece sempre seguido do nome do deputado, dessa forma também será filtrado da análise.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}}

#stopwords
library(quanteda)
stop_w <- tibble(word = stopwords(source = "stopwords-iso", language = "pt"))

#remover o nome do partido
partido <- impeachment_dilma$partido %>%
  tibble() %>%
  distinct() %>%
  rename("word" = ".") %>%
  mutate(word = str_to_lower(word))

#retirar do corpus as stopwords
tidy_impeachment <- tidy_impeachment %>% 
  anti_join(stop_w) %>%
  anti_join(partido)
```

Com a informação já no formato tidy já é possível iniciar algumas análises com os dados, como a frequência de palavras:

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
tidy_impeachment %>%
  count(word, sort = TRUE) 
```

Também pode-se visualizar as frequências geradas
```{r, eval = F, message = F}
tidy_impeachment %>%
  count(word, sort = TRUE) %>%
  mutate(word = fct_reorder(word, n)) %>%
  filter(n > 400) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(x="") 
```

Com o dado em formato Tidy, é possível comparar as palavras mais utilizadas por partido:

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
tidy_impeachment %>%
  count(word, partido, sort = TRUE) %>%
  mutate(word = fct_reorder(word, n)) %>%
  filter(n > 50) %>%
  ggplot(aes(fill = partido, x = word, y= n)) +
  geom_col() +
  coord_flip() +
  labs(x="") 
```

Avançando um pouco mais, já seria possível comparar o uso de palavras por diferentes deputados. Vamos comparar os deputados do PT, PSOL e do PSDB. Dessa forma, como o exemplo acima indica, já possuímos os dados em formato tidy, vamos selecionar os termos por partido.

                                  
```{r, results = 'hide', echo = FALSE, eval = TRUE, warning = F, message = F}

#Filtrar por partido
impeachment_pt <- impeachment_dilma %>% 
  filter(partido == "PT")

  
impeachment_psdb <- impeachment_dilma %>% 
  filter(partido == "PSDB")

impeachment_psol <- impeachment_dilma %>% 
  filter(partido == "PSOL")

#Transformando em um data frame
impeachment_pt <- tibble(line = 1:nrow(impeachment_pt),
                         text = impeachment_pt$text)  

impeachment_psdb <- tibble(line = 1:nrow(impeachment_psdb), 
                           text = impeachment_psdb$text)  

impeachment_psol <- tibble(line = 1:nrow(impeachment_psol), 
                           text = impeachment_psol$text)  


#Tokenizando

tidy_impeachment_pt <- impeachment_pt %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_w) 

tidy_impeachment_psdb <- impeachment_psdb %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_w) 
  
tidy_impeachment_psol <- impeachment_psol %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_w) 

# frequencia de palavras

frequency <- bind_rows(mutate(tidy_impeachment_psol, author = "PSOL"),
                       mutate(tidy_impeachment_psdb, author = "PSDB"),
                       mutate(tidy_impeachment_pt, author = "PT")) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `PSOL`:`PSDB`)
  
# gráfico
ggplot(frequency, aes(x = proportion, y = `PT`,
                      color = abs(`PT` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  scale_color_gradient(limits = c(0, 0.001),
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "PT", x = NULL)

```


Podemos quantificar quão diferente é o vocabulário através de um simples teste de correlação.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}

cor.test(data = frequency[frequency$author == "PSDB",],
         ~ proportion + `PT`)

```

## Corpus
O pacote `tidyverse` não possui uma função que gera corpus, assim iremos utilizar o pacote `quanteda` que permite trabalhar com Tokens, Corpus e DFMs através do seguinte fluxograma:

<center>
![quanteda_flow](images/quanteda-workflow.png){width=500px}
</center>


É possível criar um *corpus* a partir de diferentes fontes de dados:

1. Um vetor de caracteres que consista em um documento por elemento;

2. Uma matriz de dados que consista em um vetor de caracteres para os documentos e vetores adicionais para variáveis no nível do documento;

3. Um objeto de classe `VCorpus` ou `SimpleCorpus` criado pelo pacote `tm`;

4. Um objeto de palavras-chave no contexto construído por `kwic()`;

Utilizando a base de dados com as falas proferidas no dia da aprovação do impeachment da Presidenta Dilma Rousseff pela Câmara dos Deputados, nesta seção focaremos em como construir um corpus a partir de uma matriz de dados. A base de dados é a mesma utiliada no item anterior, podendo ser carregada através do pacote `text4cs`

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# carregando pacotes ----
library(quanteda)
library(readtext)
library(stringr)
library(text4cs)

impeachment_dilma <- txt4cs::impeachment_dilma
````

Inicialmente devemos transformar o data.frame em um corpus, utilizando uma função do `quanteda`

````{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}

corp <- corpus(impeachment_dilma)
summary(corp, 5)
```

### Filtrando corpus: `corpus_subset()`

A função `corpus_subset()` permite selecionar documentos em um corpus com base em variáveis no nível do documento. No caso iremos selecionar o corpus referente ao PSDB e PT.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}

corp_ptpsdb <- corpus_subset(corp, partido %in% c('PT', 'PSDB'))

summary(corp_ptpsdb, 5)

```

### Trocando a unidade de texto no corpus: `corpus_reshape()`

A função `corpus_reshape()` permite alterar a unidade de textos entre documentos, parágrafos e frases. Os textos podem ser restaurados para a unidade original mesmo que o corpus seja modificado por outras funções. 
Dessa forma, nos exemplos abaixo podemos mudar o corpus para formato de sentenças. A sentença segundo a função termina no ponto final e dá início a outra sentença.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
corp_sent <- corpus_reshape(corp, to = 'sentences')

corp_sent[11:12]
````

Ou alterá-lo para um formato de documento, restaurando-os ao formato original:

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# restaurando documentos originais ----
corp_documents <- corpus_reshape(corp_sent, to = 'documents')

corp_documents[2]

```

## Tokens e Corpus
### Obtendo Tokens a partir de um Corpus: `tokens()`

A função `tokens()`, é similar a função `unnest_tokens` do pacote `tidyverse`, sua principal diferença é blablablablabla, ambas segmentam o texto. Usaremos o corp do item anterior.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}

toks <- tokens(corp, remove_punct = TRUE) #tokenizando e removendo a pontuação
head(toks[[1]], 50)
````

### Palavras-chave e seu contexto: `kwic()`

[EXPLICAR O QUE É ISSO]

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# caso de uma palavra-chave ----
kw <- kwic(toks, pattern =  'golp*')
head(kw, 30)
````


Caso queira ler algum trecho ou os trechos que contenham a frase, você pode utilizar blablabalbalabalbala [ARRUMAR AQUI]

````{r}
texts(corp)[6]
````

PARA UMA FRASE INTEIRA BLABLABALBAAL
````{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# caso de uma frase inteira ----
kw <- kwic(toks, pattern =  phrase('não vai ter golp*'))
head(kw, 30)
```

### Selecionando tokens

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
toks <- tokens(corp, remove_punct = T, remove_numbers = T)
toks_nostop <- tokens_select(toks, pattern = stopwords('pt'), selection = 'remove')
head(toks_nostop[[1]], 50)

toks_nostop <- tokens_remove(toks_nostop, pattern = c('SR', 'PRESIDENTE', 
                                                      'Esclarecimentos'))
head(toks_nostop[[1]], 50)

```

### Gerando n-grams

Com a função `tokens_ngrams()` é possível gerar tokens com qualquer tamanho.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
toks <- tokens(corp, remove_punct = T, remove_numbers = T)
toks_ngram <- tokens_ngrams(toks, n = 2:4)
head(toks_ngram[[1]], 50)
tail(toks_ngram[[1]], 50)

# combinações específicas ----
toks_neg_bigram <- tokens_compound(toks, pattern = phrase('não *'))
head(toks_neg_bigram[[1]], 50)
toks_neg_bigram_select <- tokens_select(toks_neg_bigram, pattern = phrase('não_*'))
head(toks_neg_bigram_select[[6]], 50)

```


Retirando as stop words
```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
toks <- tokens(corp, remove_punct = T, remove_numbers = T)
toks_nostop <- tokens_select(toks, pattern = stopwords('pt'), selection = 'remove')
head(toks_nostop[[1]], 50)

toks_nostop <- tokens_remove(toks_nostop, pattern = c('SR', 'PRESIDENTE', 
                                                      'Esclarecimentos'))
head(toks_nostop[[1]], 50)

```



## DFM: Matriz de documentos e termos
A função `dfm()` constrói uma matriz de documentos e termos/palavras/stems/tokens (DFM) a partir de um objeto de tokens.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = T)
toks <- tokens_select(toks, pattern = stopwords('pt'), selection = 'remove')

dfmat <- dfm(toks)

ndoc(dfmat)  # numero de documentos
nfeat(dfmat)  # numero de features
head(docnames(dfmat), 20)  # ids dos documentos
head(featnames(dfmat), 20)  # algumas features
topfeatures(dfmat, 10)  # features mais frequentes

```

### Refinando a seleção de features

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}

# selecionando tokes que aparecem numa proporção específica de documentos ----
dfmat_docfreq <- dfm_trim(dfmat, min_docfreq = 0.01, docfreq_type = "prop")

nfeat(dfmat)
nfeat(dfmat_docfreq)
```

### Agrupando documentos numa DFM

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
head(docvars(dfmat_docfreq))

# por autor ----
dfmat_autor <- dfm(dfmat_docfreq, groups = "nomeOrador")
ndoc(dfmat_autor)
head(docvars(dfmat_autor))

# por partido ----
dfmat_party <- dfm(dfmat_docfreq, groups = "partido")
ndoc(dfmat_party)
docvars(dfmat_party)
```

## Stemming
Até aqui, mesmo que os procedimentos adotados tenham diminuído a dimensionalidade do acervo de documentos ao transformá-lo em uma sacola de palavras (*bag of words*), ainda é necessária a adoção de procedimentos que possam reduzir a complexidade do conteúdo a ser analisado.

Com esse objetivo, como apontado por Izumi e Moreira (2018) podemos pensar que determinado documento tenha em sua composição as seguintes palavras únicas: trabalho, trabalhador, trabalhista. Apesar de seus diferentes signficados, cada uma dessas palavras pode
ser reduzida ao seu radical, _trabalh_, dando ao pequisador informações suficientes para sua
análise e, assim reduzindo, o _n_ de três palavras únicas para uma palavra que tem a soma das frequências anteriores. Para garantir que palavras que variam apenas na flexão, número ou conjugação sejam consideradas iguais, reduzindo o número de dimensões contido no acervo, por meio da adaptação do algoritmo de Porter (1980) para o português já desenvolvida por diferentes projetos ([\textcolor{blue}{Snowball}](http://snowball.tartarus.org/) e 
[\textcolor{blue}{NILC-USP}](https://bit.ly/2OXkVS6)),podem ser obtidos os _stems_ das palavras restantes^[O processo de stemming é uma aproximação do processo de lematização, que reduz palavras às suas formas básicas.]. 

O pacote `quanteda` possui a opção de stemming da DFM em português que usa o projeto Snowball através da função `dfm_wordstem()`. Contudo obtive erro ao implementá-la. Desse modo, vamos aplicar outro processo e, posteriormente, retornar para a estrutura do pacote.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# funções ----
remove_acento <- function(vec, Toupper=F) {
  # Função desenvolvida por Manoel Galdino e aprimorada por Davi Moreira
  # Recebe um vetor com strings e remove os acentos com caracteres em letras 
  # minúsculas por padrão
  vec <- tolower(vec)
  vec <- gsub('á', 'a', vec) 
  vec <- gsub('à', 'a', vec)
  vec <- gsub('â', 'a', vec)
  vec <- gsub('ã', 'a', vec)
  vec <- gsub('â', 'a', vec)
  vec <- gsub('é', 'e', vec) 
  vec <- gsub('è', 'e', vec)
  vec <- gsub('ê', 'e', vec)
  vec <- gsub('í', 'i', vec)
  vec <- gsub('õ', 'o', vec) 
  vec <- gsub('ó', 'o', vec)
  vec <- gsub('ô', 'o', vec)
  vec <- gsub('ò', 'o', vec)
  vec <- gsub('ú', 'u', vec)
  vec <- gsub('ç', 'c', vec)
  vec <- gsub('\'', '', vec)
  if ( Toupper==T) vec <- toupper(vec)
  return(vec)
}
# pacotes ----
if(require(tm) == F) install.packages('tm'); require(tm);

# processando dados ----
load("./dados/impeachment-dilma-dados-completo.RData")
disc <- bdDados$discursoPlainTxt  # vetor de strings
# removendo acento
disc <- remove_acento(disc, Toupper = F)  
# head(disc)
# removendo stop words - lista pessoal
load("./dados/stopWordsPTBRA.RData")  # lista pessoal
disc <- strsplit(disc, " ", fixed = T, perl = F, useBytes = FALSE)  
disc <- lapply(disc, function(x) paste(" ", x, " ", sep = ""))  # 
disc <- lapply(disc, function(x) ifelse(x %in% stopW, "", x))  # fast gsub
disc <- lapply(disc, paste, collapse = " ")
disc <- unlist(disc)  
# head(disc)

# transformando vetor de documentos em corpus
disc <- Corpus(VectorSource(disc), 
                            readerControl = list(language = "portuguese"))
# stemming
disc <- tm_map(disc, stemDocument, 
                            language = "portuguese") # obtendo stem

# retomando vetor de strings
disc <- data.frame(text = unlist(disc), 
                    stringsAsFactors=F)
disc <- disc$text[-length(disc$text)]
# disc[1]

# adicionando nova coluna na base de dados
bdDados$text <- disc

# doc_id
bdDados$doc_id <- c(1:dim(bdDados)[1])

# partidos
names(bdDados) <- ifelse(names(bdDados) == "paritdo", "partido", names(bdDados))

# criando corpus pacote quanteda ----
corp <- corpus(bdDados)
summary(corp, 5)

dfm <- corp %>% 
  tokens(remove_punct = TRUE, remove_numbers = T) %>% 
  tokens_remove(pattern = c("sr", "sras", "srs", "deput", "total", "vot", "votos", 
                            "bet", "bloco", "mansur", "president", "presidente", 
                            "palmas", "plenario", "aqu", "estad")) %>% 
  dfm() %>% 
  dfm_select(min_nchar = 2) %>%
  dfm_trim(min_docfreq = 0.01, docfreq_type = "prop") # %>%
  # dfm_wordstem(language = quanteda_options("portuguese"))

topfeatures(dfm, 10)
docnames(dfm)
featnames(dfm)
  
```



## FCM: Matriz de co-ocorrência de termos
É possível construir uma FCM a partir de uma DFM ou um objeto de tokens usando a função `fcm()`. `topfeatures()` retorna as palavras que ocorrem mais freqüentemente. A FCM pode ser usado para visualizar rede semântica com a função `textplot_network()`, por exemplo.

```{r, results = 'hide', echo = TRUE, eval = F, warning = F, message = F}
# carregando pacotes ----
if(require(text2vec) == F) install.packages('text2vec'); require(text2vec);

fcmat <- fcm(dfm)
dim(fcmat)

feat <- names(topfeatures(fcmat, 50))
fcmat_select <- fcm_select(fcmat, pattern = feat)
dim(fcmat_select)

size <- log(colSums(dfm_select(dfmat, feat)))
set.seed(144)
textplot_network(fcmat_select, #min_freq = 0.2, 
                 vertex_size = size / max(size) * 3)

```



<!--chapter:end:06-wrangling.Rmd-->

# Mineração e estatísticas básicas {#stat}

EM CONSTRUÇÃO...

## Análise de frequência

## Nuvem de palavras

## tf-idf

## Rede de n-grams

## Correlação pareada

## Diversidade lexical

## Similaridade entre documentos/termos

## KEYNESS: Análise de Frequência Relativa

<!--chapter:end:07-statistics.Rmd-->

# Escalonamento {#scalling}

EM CONSTRUÇÃO...

## Wordscore

## Wordfish


<!--chapter:end:08-escalonamento.Rmd-->

# Classificação {#classificacao}

EM CONSTRUÇÃO...

## Método de dicionário: Análise de sentimento

## Naive Bayes

## LDA: Latent Dirichlet Allocation 

## STM: Structed Topic Model

<!--chapter:end:09-classificacao.Rmd-->

`r if (knitr::is_html_output()) '
# Referências {-}
'`

<!--chapter:end:99-references.Rmd-->

